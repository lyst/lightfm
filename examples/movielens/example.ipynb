{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using lightfm on the Movielens dataset\n",
    "\n",
    "## Getting the data\n",
    "The first step is to get the movielens data.\n",
    "\n",
    "Let's import the utility functions from `data.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions get the dataset, and save it to a local file, and parse it into sparse matrices we can pass into `LightFM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, `_build_interaction_matrix` constructs the interaction matrix: a (no_users, no_items) matrix with 1 in place of positive interactions, and -1 in place of negative interactions. For this experiment, any rating lower than 4 is a negative rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _build_interaction_matrix(rows, cols, data):\n",
      "    \"\"\"\n",
      "    Build the training matrix (no_users, no_items),\n",
      "    with ratings >= 4.0 being marked as positive and\n",
      "    the rest as negative.\n",
      "    \"\"\"\n",
      "\n",
      "    mat = sp.lil_matrix((rows, cols), dtype=np.int32)\n",
      "\n",
      "    for uid, iid, rating, timestamp in data:\n",
      "        if rating >= 4.0:\n",
      "            mat[uid, iid] = 1.0\n",
      "        else:\n",
      "            mat[uid, iid] = -1.0\n",
      "\n",
      "    return mat.tocoo()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(data._build_interaction_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it! The dataset will be automatically downloaded and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = data.get_movielens_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<944x1683 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 90570 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<944x1683 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 9430 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good and ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "Let's import the lightfm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LightFM(no_components=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we set the latent dimensionality of the model to 30. Fitting is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7f035fac4a10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a handle on the model accuracy using the ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_predictions = model.predict(train.row,\n",
    "                                  train.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.16739023, -1.81594849,  0.33082035, ..., -0.84471774,\n",
       "       -3.59646535, -2.3761344 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98794140543734321"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(train.data, train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got very high accuracy on the train dataset; let's check the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test.row, test.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72314175941707015"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test.data, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is much lower on the test data, suggesting a high degree of overfitting. We can combat this by regularizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76055575505353468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(no_components=30, user_alpha=0.0001, item_alpha=0.0001)\n",
    "model.fit(train, epochs=50)\n",
    "roc_auc_score(test.data, model.predict(test.row, test.col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modicum of regularization gives much better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using metadata\n",
    "The promise of `lightfm` is the possibility of using metadata in cold-start scenarios. The Movielens dataset has genre data for the movies it contains. Let's use that to train the `LightFM` model.\n",
    "\n",
    "The `get_movielens_item_metadata` function constructs a (no_items, no_features) matrix containing features for the movies; if we use genres this will be a (no_items, no_genres) feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1683x19 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 2893 stored elements in LInked List format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features = data.get_movielens_item_metadata(use_item_ids=False)\n",
    "item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass these to the `fit` method in order to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67300181616251231"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(no_components=30, user_alpha=0.0001, item_alpha=0.0001)\n",
    "model.fit(train, item_features=item_features, epochs=50)\n",
    "roc_auc_score(test.data, model.predict(test.row, test.col, item_features=item_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not as accurate as a pure collaborative filtering solution, but should enable us to make recommendations new movies.\n",
    "\n",
    "If we add item-specific features back, we should get the original accuracy back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1683x1683 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 4572 stored elements in LInked List format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features = data.get_movielens_item_metadata(use_item_ids=True)\n",
    "item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75583737010915852"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFM(no_components=30, user_alpha=0.0001, item_alpha=0.0001)\n",
    "model.fit(train, item_features=item_features, epochs=50)\n",
    "roc_auc_score(test.data, model.predict(test.row, test.col, item_features=item_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implicit feedback\n",
    "So far, we have been treating the signals from the data as binary explicit feedback: either a user likes a movie (score >= 4) or does not. However, in many applications feedback is purely implicit: the items a user interacted with are positive signals, but we have no negative signals.\n",
    "\n",
    "`lightfm` implements two models suitable for dealing with this sort of data:\n",
    "\n",
    "- BPR: Bayesian Personalised Ranking [1] pairwise loss. Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
    "- WARP: Weighted Approximate-Rank Pairwise [2] loss. Maximises the rank of positive examples by repeatedly sampling negative examples until rank violating one is found. Useful when only positive interactions are present and optimising the top of the recommendation list (precision@k) is desired.\n",
    "\n",
    "[1] Rendle, Steffen, et al. \"BPR: Bayesian personalized ranking from implicit feedback.\" Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009.\n",
    "\n",
    "[2] Weston, Jason, Samy Bengio, and Nicolas Usunier. \"Wsabie: Scaling up to large vocabulary image annotation.\" IJCAI. Vol. 11. 2011.\n",
    "\n",
    "Before using them, let's first load the data and define some evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = data.get_movielens_data()\n",
    "train.data = np.ones_like(train.data)\n",
    "test.data = np.ones_like(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def precision_at_k(model, ground_truth, k):\n",
    "    \"\"\"\n",
    "    Measure precision at k for model and ground truth.\n",
    "\n",
    "    Arguments:\n",
    "    - lightFM instance model\n",
    "    - sparse matrix ground_truth (no_users, no_items)\n",
    "    - int k\n",
    "\n",
    "    Returns:\n",
    "    - float precision@k\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth = ground_truth.tocsr()\n",
    "\n",
    "    no_users, no_items = ground_truth.shape\n",
    "\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for user_id, row in enumerate(ground_truth):\n",
    "        uid_array = np.empty(no_items, dtype=np.int32)\n",
    "        uid_array.fill(user_id)\n",
    "        predictions = model.predict(uid_array, pid_array, num_threads=4)\n",
    "\n",
    "        top_k = set(np.argsort(-predictions)[:k])\n",
    "        true_pids = set(row.indices[row.data == 1])\n",
    "\n",
    "        if true_pids:\n",
    "            precisions.append(len(top_k & true_pids) / float(k))\n",
    "\n",
    "    return sum(precisions) / len(precisions)\n",
    "\n",
    "\n",
    "def full_auc(model, ground_truth):\n",
    "    \"\"\"\n",
    "    Measure AUC for model and ground truth on all items.\n",
    "\n",
    "    Arguments:\n",
    "    - lightFM instance model\n",
    "    - sparse matrix ground_truth (no_users, no_items)\n",
    "\n",
    "    Returns:\n",
    "    - float AUC\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth = ground_truth.tocsr()\n",
    "\n",
    "    no_users, no_items = ground_truth.shape\n",
    "\n",
    "    pid_array = np.arange(no_items, dtype=np.int32)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for user_id, row in enumerate(ground_truth):\n",
    "        uid_array = np.empty(no_items, dtype=np.int32)\n",
    "        uid_array.fill(user_id)\n",
    "        predictions = model.predict(uid_array, pid_array, num_threads=4)\n",
    "\n",
    "        true_pids = row.indices[row.data == 1]\n",
    "\n",
    "        grnd = np.zeros(no_items, dtype=np.int32)\n",
    "        grnd[true_pids] = 1\n",
    "\n",
    "        if len(true_pids):\n",
    "            scores.append(roc_auc_score(grnd, predictions))\n",
    "\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a BPR model and look at its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.421208907741, 0.0622481442206\n",
      "AUC: 0.838544064431, 0.819069444911\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(learning_rate=0.05, loss='bpr')\n",
    "\n",
    "model.fit_partial(train,\n",
    "                  epochs=10)\n",
    "\n",
    "train_precision = precision_at_k(model,\n",
    "                                 train,\n",
    "                                 10)\n",
    "test_precision = precision_at_k(model,\n",
    "                                test,\n",
    "                                10)\n",
    "\n",
    "train_auc = full_auc(model, train)\n",
    "test_auc = full_auc(model, test)\n",
    "\n",
    "print('Precision: %s, %s' % (train_precision, test_precision))\n",
    "print('AUC: %s, %s' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WARP model, on the other hand, optimises for precision@k---we should expect its performance to be better on precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.624708377519, 0.110816542948\n",
      "AUC: 0.941236748837, 0.904416726513\n"
     ]
    }
   ],
   "source": [
    "model = LightFM(learning_rate=0.05, loss='warp')\n",
    "\n",
    "model.fit_partial(train,\n",
    "                  epochs=10)\n",
    "\n",
    "train_precision = precision_at_k(model,\n",
    "                                 train,\n",
    "                                 10)\n",
    "test_precision = precision_at_k(model,\n",
    "                                test,\n",
    "                                10)\n",
    "\n",
    "train_auc = full_auc(model, train)\n",
    "test_auc = full_auc(model, test)\n",
    "    \n",
    "print('Precision: %s, %s' % (train_precision, test_precision))\n",
    "print('AUC: %s, %s' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is exactly what we see: we get much higher precision@10 (but the AUC metric is also improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
